{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/exobyte-labs/betamark.git\n",
    "!pip install genomic-benchmarks torch\n",
    "!pip install scikit-learn\n",
    "!pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Main library for building and training neural networks\n",
    "import torch.nn as nn  # Provides essential neural network layers\n",
    "import torch.optim as optim  # Contains optimizers to update model parameters\n",
    "from torch.utils.data import DataLoader  # For batching and shuffling datasets\n",
    "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanOcrEnsembl  # Genomic dataset\n",
    "from betamark import ocr  # OCR model, potentially for additional text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = HumanOcrEnsembl(split='train', version=0)\n",
    "test_dset = HumanOcrEnsembl(split='test', version=0)\n",
    "\n",
    "train_loader = DataLoader(train_dset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(GenomicCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Convolutional layers with increased filter sizes\n",
    "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=128, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer with 50% dropout rate\n",
    "        \n",
    "        # Fully connected layer adjusted for the convolutional output dimensions\n",
    "        self.fc = nn.Linear(256 * ((500 - 5 + 1) // 2 - 5 + 1) // 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the model layers\n",
    "        x = self.embedding(x).permute(0, 2, 1)  # Embedding followed by permutation for Conv1d input\n",
    "        x = self.dropout(self.pool1(torch.relu(self.conv1(x))))\n",
    "        x = self.dropout(self.pool2(torch.relu(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten for the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)  # Sigmoid activation for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'A': 1, 'C': 2, 'G': 3, 'T': 4, 'N': 0}  # Vocabulary mapping for nucleotide encoding\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 200  # Increased embedding dimension for richer feature representation\n",
    "num_classes = 1  # Binary classification\n",
    "\n",
    "# Model instantiation and device assignment\n",
    "model = GenomicCNN(vocab_size, embed_dim, num_classes).to(device)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_tensor(sequence, vocab, max_length=500):\n",
    "    indices = [vocab.get(char, 0) for char in sequence]  # Encoding sequence based on vocabulary\n",
    "    # Padding or trimming to a fixed length\n",
    "    if len(indices) < max_length:\n",
    "        indices += [0] * (max_length - len(indices))\n",
    "    elif len(indices) > max_length:\n",
    "        indices = indices[:max_length]\n",
    "    return torch.tensor(indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6364, Accuracy: 63.40%\n",
      "New best model saved with accuracy: 63.40%\n",
      "Epoch 2/20, Loss: 0.6295, Accuracy: 64.28%\n",
      "New best model saved with accuracy: 64.28%\n",
      "Epoch 3/20, Loss: 0.6236, Accuracy: 64.88%\n",
      "New best model saved with accuracy: 64.88%\n",
      "Epoch 4/20, Loss: 0.6203, Accuracy: 65.26%\n",
      "New best model saved with accuracy: 65.26%\n",
      "Epoch 5/20, Loss: 0.6183, Accuracy: 65.50%\n",
      "New best model saved with accuracy: 65.50%\n",
      "Epoch 6/20, Loss: 0.6164, Accuracy: 65.74%\n",
      "New best model saved with accuracy: 65.74%\n",
      "Epoch 7/20, Loss: 0.6147, Accuracy: 65.82%\n",
      "New best model saved with accuracy: 65.82%\n",
      "Epoch 8/20, Loss: 0.6138, Accuracy: 65.93%\n",
      "New best model saved with accuracy: 65.93%\n",
      "Epoch 9/20, Loss: 0.6125, Accuracy: 66.01%\n",
      "New best model saved with accuracy: 66.01%\n",
      "Epoch 10/20, Loss: 0.6119, Accuracy: 66.22%\n",
      "New best model saved with accuracy: 66.22%\n",
      "Epoch 11/20, Loss: 0.6114, Accuracy: 66.15%\n",
      "Epoch 12/20, Loss: 0.6097, Accuracy: 66.34%\n",
      "New best model saved with accuracy: 66.34%\n",
      "Epoch 13/20, Loss: 0.6086, Accuracy: 66.47%\n",
      "New best model saved with accuracy: 66.47%\n",
      "Epoch 14/20, Loss: 0.6078, Accuracy: 66.61%\n",
      "New best model saved with accuracy: 66.61%\n",
      "Epoch 15/20, Loss: 0.6082, Accuracy: 66.38%\n",
      "Epoch 16/20, Loss: 0.6073, Accuracy: 66.57%\n",
      "Epoch 17/20, Loss: 0.6073, Accuracy: 66.55%\n",
      "Epoch 18/20, Loss: 0.6057, Accuracy: 66.65%\n",
      "New best model saved with accuracy: 66.65%\n",
      "Epoch 19/20, Loss: 0.6064, Accuracy: 66.71%\n",
      "New best model saved with accuracy: 66.71%\n",
      "Epoch 20/20, Loss: 0.6043, Accuracy: 66.81%\n",
      "New best model saved with accuracy: 66.81%\n"
     ]
    }
   ],
   "source": [
    "epochs = 20  # Number of training epochs\n",
    "best_accuracy = 0  # Initialize best accuracy to track the best model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        # Convert sequences to padded tensors\n",
    "        x = torch.stack([sequence_to_tensor(seq, vocab) for seq in x]).to(device)\n",
    "        y = y.to(device).float()  # Move labels to device and convert to float for BCE loss\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.unsqueeze(1))  # Calculate loss\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "        correct += ((output > 0.5).float() == y.unsqueeze(1)).sum().item()  # Accuracy calculation\n",
    "        total += y.size(0)\n",
    "\n",
    "    # Calculate and print epoch loss and accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Check if this epoch's accuracy is the best so far\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), \"best_genomic_cnn_model.pth\")  # Save the best model state dict\n",
    "        print(f\"New best model saved with accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(total_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 620.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.5}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from betamark import ocr\n",
    "\n",
    "# Define the prediction function for OCR binary classification\n",
    "def model_predict(sequence):\n",
    "    # Convert the sequence to a tensor and add a batch dimension\n",
    "    input_tensor = sequence_to_tensor(sequence, vocab).unsqueeze(0).to(device)  # [1, max_length]\n",
    "    \n",
    "    # Set model to evaluation mode and disable gradient tracking\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    \n",
    "    # Convert output to binary prediction (0 or 1)\n",
    "    y_pred = int((output > 0.5).item())  # Output is 0 if not OCR, 1 if OCR\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Run the OCR evaluation using `betamark`\n",
    "ocr.run_eval(user_func=model_predict)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
